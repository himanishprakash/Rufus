{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Web Crawler\n",
      "--------------------------------------------------\n",
      "\n",
      "Starting crawl...\n"
     ]
    },
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 334\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrawl completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 334\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nest_asyncio.py:31\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nest_asyncio.py:99\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:256\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[21], line 329\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    326\u001b[0m max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter maximum crawl depth (default 2): \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting crawl...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 329\u001b[0m crawler \u001b[38;5;241m=\u001b[39m \u001b[43mRufus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m crawler\u001b[38;5;241m.\u001b[39mcrawl(base_url, instruction, max_depth)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrawl completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 36\u001b[0m, in \u001b[0;36mRufus.__init__\u001b[0;34m(self, api_key)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, api_key: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    Initialize the semantic web crawler.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m        api_key (str): OpenAI API key for semantic analysis\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisited_urls: Set[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_relevance: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_client.py:105\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    103\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import Dict, List, Set, Any\n",
    "from openai import OpenAI\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('semantic_crawler.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class Rufus:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"\n",
    "        Initialize the semantic web crawler.\n",
    "        \n",
    "        Args:\n",
    "            api_key (str): OpenAI API key for semantic analysis\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.visited_urls: Set[str] = set()\n",
    "        self.page_relevance: Dict[str, bool] = {}\n",
    "        self.page_data: Dict[str, Dict[str, Any]] = {}\n",
    "        self.depth_data: Dict[int, List[str]] = {}\n",
    "        self.keywords: List[str] = []\n",
    "\n",
    "    async def get_semantic_keywords(self, instruction: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate semantically related keywords for the search instruction.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        prompt = f\"\"\"Generate semantically related keywords and phrases for this instruction:\n",
    "        \n",
    "        Instruction: \"{instruction}\"\n",
    "        \n",
    "        Consider:\n",
    "        1. Direct synonyms and related terms\n",
    "        2. Industry-specific terminology\n",
    "        3. Common abbreviations\n",
    "        4. Related concepts and topics\n",
    "        5. Contextual variations\n",
    "        \n",
    "        Return ONLY a comma-separated list of keywords, no explanations.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"o1-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a semantic analysis expert. Return only a comma-separated list of keywords.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            keywords = [kw.strip() for kw in response.choices[0].message.content.split(',')]\n",
    "            logging.info(f\"Generated keywords: {keywords}\")\n",
    "            return keywords\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating keywords: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def is_content_relevant(self, content: str, instruction: str, keywords: List[str]) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if page content is relevant using semantic analysis.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        prompt = f\"\"\"Analyze if this content is relevant to the instruction and keywords.\n",
    "\n",
    "        Instruction: \"{instruction}\"\n",
    "        Keywords: {', '.join(keywords)}\n",
    "        \n",
    "        Content (excerpt):\n",
    "        \\\"\\\"\\\"\n",
    "        {content[:2000]}\n",
    "        \\\"\\\"\\\"\n",
    "        \n",
    "        Consider:\n",
    "        1. Direct keyword matches\n",
    "        2. Semantic relationship to instruction\n",
    "        3. Context and meaning\n",
    "        4. Content quality and depth\n",
    "        5. Information value\n",
    "        \n",
    "        Answer ONLY with TRUE or FALSE.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a content relevance analyst. Respond only with TRUE or FALSE.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            is_relevant = response.choices[0].message.content.strip().upper() == \"TRUE\"\n",
    "            logging.info(f\"Content relevance: {is_relevant}\")\n",
    "            return is_relevant\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error checking content relevance: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    async def should_follow_link(self, link_text: str, href: str, \n",
    "                               instruction: str, keywords: List[str], \n",
    "                               surrounding_text: str = \"\") -> bool:\n",
    "        \"\"\"\n",
    "        Determine if a link should be followed based on semantic analysis.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        prompt = f\"\"\"Should we follow this link based on the instruction and context?\n",
    "        \n",
    "        Instruction: \"{instruction}\"\n",
    "        Keywords: {', '.join(keywords)}\n",
    "        \n",
    "        Link Analysis:\n",
    "        - Link Text: \"{link_text}\"\n",
    "        - URL: {href}\n",
    "        - Surrounding Context: \"{surrounding_text}\"\n",
    "        \n",
    "        Consider:\n",
    "        1. Relevance to instruction\n",
    "        2. Keyword matches\n",
    "        3. URL structure/path\n",
    "        4. Link context\n",
    "        5. Potential information value\n",
    "        \n",
    "        Answer ONLY with TRUE or FALSE.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a link relevance analyst. Respond only with TRUE or FALSE.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            should_follow = response.choices[0].message.content.strip().upper() == \"TRUE\"\n",
    "            logging.info(f\"Should follow {href}: {should_follow}\")\n",
    "            return should_follow\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error checking link relevance: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    async def analyze_page_links(self, page, current_url: str, base_url: str, \n",
    "                               instruction: str) -> Dict[str, bool]:\n",
    "        \"\"\"\n",
    "        Analyze all links on the current page for relevance.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        link_relevance = {}\n",
    "        try:\n",
    "            links = await page.query_selector_all('a[href]')\n",
    "            for link in links:\n",
    "                href = await link.get_attribute('href')\n",
    "                if href:\n",
    "                    absolute_url = urljoin(current_url, href)\n",
    "                    if absolute_url.startswith(base_url) and absolute_url not in self.visited_urls:\n",
    "                        link_text = await link.inner_text()\n",
    "                        parent = await link.evaluate('element => element.parentElement.textContent')\n",
    "                        surrounding_text = parent[:200] if parent else \"\"\n",
    "                        \n",
    "                        is_relevant = await self.should_follow_link(\n",
    "                            link_text, absolute_url, instruction, \n",
    "                            self.keywords, surrounding_text\n",
    "                        )\n",
    "                        link_relevance[absolute_url] = is_relevant\n",
    "            \n",
    "            return link_relevance\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in link analysis: {str(e)}\")\n",
    "            return link_relevance\n",
    "\n",
    "    async def crawl_page(self, page, current_url: str, base_url: str, instruction: str, \n",
    "                        depth: int = 0, max_depth: int = 2):\n",
    "        \"\"\"\n",
    "        Crawl a page and analyze its content and links.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        if depth > max_depth or current_url in self.visited_urls:\n",
    "            return\n",
    "            \n",
    "        self.visited_urls.add(current_url)\n",
    "        \n",
    "        if depth not in self.depth_data:\n",
    "            self.depth_data[depth] = []\n",
    "        \n",
    "        try:\n",
    "            # Generate keywords if not already done\n",
    "            if not self.keywords:\n",
    "                self.keywords = await self.get_semantic_keywords(instruction)\n",
    "            \n",
    "            await page.goto(current_url)\n",
    "            logging.info(f\"Crawling depth {depth}: {current_url}\")\n",
    "            \n",
    "            await page.wait_for_load_state('networkidle')\n",
    "            await asyncio.sleep(1)\n",
    "            \n",
    "            # Get and analyze content\n",
    "            content = await page.inner_text('body')\n",
    "            is_relevant = self.is_content_relevant(content, instruction, self.keywords)\n",
    "            self.page_relevance[current_url] = is_relevant\n",
    "            \n",
    "            # Store relevant page data\n",
    "            if is_relevant:\n",
    "                self.page_data[current_url] = {\n",
    "                    'url': current_url,\n",
    "                    'depth': depth,\n",
    "                    'content': content,\n",
    "                    'crawl_time': datetime.now().isoformat(),\n",
    "                    'title': await page.title(),\n",
    "                    'matched_keywords': self.keywords\n",
    "                }\n",
    "                self.depth_data[depth].append(current_url)\n",
    "            \n",
    "            # Analyze and follow links if not at max depth\n",
    "            if depth < max_depth:\n",
    "                link_relevance = await self.analyze_page_links(page, current_url, base_url, instruction)\n",
    "                \n",
    "                for url, relevant in link_relevance.items():\n",
    "                    if relevant and url not in self.visited_urls:\n",
    "                        await self.crawl_page(\n",
    "                            page, url, base_url, instruction, \n",
    "                            depth + 1, max_depth\n",
    "                        )\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error crawling {current_url}: {str(e)}\")\n",
    "\n",
    "    def save_results(self, base_url: str, instruction: str):\n",
    "        \"\"\"\n",
    "        Save crawl results to JSON file.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f'semantic_results_{timestamp}.json'\n",
    "        \n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'base_url': base_url,\n",
    "                'instruction': instruction,\n",
    "                'crawl_time': datetime.now().isoformat(),\n",
    "                'total_pages': len(self.visited_urls),\n",
    "                'relevant_pages': sum(1 for v in self.page_relevance.values() if v),\n",
    "                'keywords_used': self.keywords\n",
    "            },\n",
    "            'relevance_map': self.page_relevance,\n",
    "            'depth_analysis': {\n",
    "                str(depth): {\n",
    "                    'urls': urls,\n",
    "                    'count': len(urls)\n",
    "                } for depth, urls in self.depth_data.items()\n",
    "            },\n",
    "            'page_data': self.page_data\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        logging.info(f\"Results saved to {filename}\")\n",
    "        return filename\n",
    "\n",
    "    async def crawl(self, base_url: str, instruction: str, max_depth: int = 2):\n",
    "        \"\"\"\n",
    "        Main crawling function.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        if not base_url.startswith(('http://', 'https://')):\n",
    "            base_url = 'https://' + base_url\n",
    "        if not base_url.endswith('/'):\n",
    "            base_url += '/'\n",
    "            \n",
    "        async with async_playwright() as p:\n",
    "            browser = await p.chromium.launch(headless=True)\n",
    "            page = await browser.new_page()\n",
    "            \n",
    "            try:\n",
    "                await self.crawl_page(page, base_url, base_url, instruction, max_depth=max_depth)\n",
    "                filename = self.save_results(base_url, instruction)\n",
    "                logging.info(f\"Crawl completed successfully. Results saved to {filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Crawl failed: {str(e)}\")\n",
    "            finally:\n",
    "                await browser.close()\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function to run the crawler.\n",
    "    \"\"\"\n",
    "    print(\"Semantic Web Crawler\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    base_url = input(\"Enter the base URL to crawl: \").strip()\n",
    "    instruction = input(\"Enter your search instruction: \").strip()\n",
    "    max_depth = int(input(\"Enter maximum crawl depth (default 2): \") or \"2\")\n",
    "    \n",
    "    print(\"\\nStarting crawl...\")\n",
    "    crawler = Rufus(api_key)\n",
    "    await crawler.crawl(base_url, instruction, max_depth)\n",
    "    print(\"Crawl completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://www.sf.gov/\n",
    "\n",
    "We're making a chatbot for the HR in San Francisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rufus import Rufus\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
